¿Qué es LangChain (en el contexto de este tutorial)?<tab>Un framework/biblioteca que simplifica la construcción de aplicaciones con Modelos de Lenguaje (LLMs), actuando como un "pegamento" y "organizador" de componentes.
¿Qué es un LLM (Modelo de Lenguaje) en el contexto de LangChain?<tab>El "cerebro" o "motor" de IA que entiende y genera texto. LangChain ayuda a interactuar con él. (Ej: GPT, Gemini).
¿Para qué se usa una función como init_chat_model en LangChain (según el ejemplo del tutorial)?<tab>Para inicializar y configurar el Modelo de Chat específico (LLM) que se va a utilizar en la aplicación, seleccionando el proveedor y el modelo.
¿Qué son los Messages en LangChain al interactuar con un modelo de chat?<tab>Objetos estructurados que representan los diferentes turnos o partes de una conversación/instrución para el LLM.
¿Cuál es el propósito de un SystemMessage en LangChain?<tab>Proporcionar instrucciones generales, contexto o el rol que debe asumir el LLM. (Ej: "Traduce de inglés a X").
¿Cuál es el propósito de un HumanMessage en LangChain?<tab>Representar la entrada o pregunta específica del usuario que el LLM debe procesar. (Ej: "hi!").
¿Qué hace el método .invoke() en un objeto modelo de LangChain?<tab>Envía la entrada (generalmente una lista de Messages o un PromptValue) al LLM y devuelve la respuesta completa de forma síncrona.
¿Qué permite el método .stream() en un objeto modelo de LangChain?<tab>Recibir la respuesta del LLM en fragmentos (tokens) a medida que se generan, en lugar de esperar la respuesta completa. Útil para interfaces interactivas.
¿Qué es un ChatPromptTemplate en LangChain?<tab>Una herramienta para crear plantillas de prompts (instrucciones) dinámicas y reutilizables para modelos de chat, permitiendo insertar variables.
¿Cómo se usan las variables (ej: {language}, {text}) dentro de un ChatPromptTemplate?<tab>Como marcadores de posición (placeholders) que se rellenarán con valores específicos cuando se utilice la plantilla, permitiendo personalizar el prompt.
¿Para qué se utiliza ChatPromptTemplate.from_messages(...) en LangChain?<tab>Para construir un ChatPromptTemplate definiendo una secuencia de mensajes (con sus roles y contenido, incluyendo variables) que conformarán la estructura del prompt final.
¿Qué ocurre al llamar a .invoke() en un objeto ChatPromptTemplate pasándole un diccionario (ej: prompt_template.invoke({"language": "Italian", "text": "hi!"}))?<tab>La plantilla se formatea: las variables en la plantilla se reemplazan con los valores del diccionario, generando un PromptValue (un conjunto de mensajes formateados) listo para ser enviado al LLM.
¿Cuál es el flujo básico de una aplicación simple con LangChain usando un modelo y una plantilla de prompt?<tab>1. Recibir entrada del usuario. 2. Usar ChatPromptTemplate.invoke() con la entrada para formatear el prompt. 3. Pasar el prompt formateado al model.invoke() para obtener la respuesta del LLM.
¿Qué rol principal cumple LangChain al facilitar la interacción con LLMs?<tab>Actúa como una capa de abstracción, proporcionando una interfaz estándar y herramientas para interactuar con diferentes LLMs y construir flujos de trabajo (como el uso de plantillas de prompt).
¿Qué tipo de objeto devuelve prompt_template.invoke(...)?<tab>Un PromptValue, que es una representación del prompt formateado, usualmente conteniendo una lista de Message objects.
¿Qué tipo de objeto es la entrada principal para el método model.invoke() cuando se usan modelos de chat?<tab>Una lista de objetos Message (como SystemMessage, HumanMessage) o un PromptValue que se pueda convertir a ellos.